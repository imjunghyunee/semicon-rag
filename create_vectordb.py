import json
# from langchain.document_loaders import TextLoader
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
# from langchain.vectorstores import FAISS
from langchain_community.vectorstores import FAISS
#from langchain.embeddings import HuggingFaceEmbeddings

from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer
import torch

# 1. Markdown íŒŒì¼ ë¡œë“œ
file_path = "./data/ai.md"
loader = TextLoader(file_path, encoding="utf-8")  # ì¸ì½”ë”© ë¬¸ì œ ì—†ë„ë¡ ëª…ì‹œ
documents = loader.load()

# docsì—ì„œ raw text ì¶”ì¶œ
raw_text = documents[0].page_content

# 2. Markdown í—¤ë” ê¸°ì¤€ Split
headers_to_split_on = [("#", "Header1"), ("##", "Header2")]
splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False,
)
split_docs = splitter.split_text(raw_text)


for doc in split_docs:
    print(f"{doc.page_content}")
    print(f"{doc.metadata}", end="\n=====================\n")

# 3. HuggingFace ì„ë² ë”© ëª¨ë¸ ì •ì˜
embedding_model = HuggingFaceEmbeddings(
    model_name="jinaai/jina-embeddings-v3",
    model_kwargs={
        "trust_remote_code": True,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
    },
)
# 4. FAISS vector DB ìƒì„±
vectordb = FAISS.from_documents(split_docs, embedding_model)

# 5. ì €ì¥ (ì„ íƒ)
vectordb.save_local("./vectordb/faiss")

print("Successfully created and saved the FAISS vector database.")


# 5. Tokenizer ë¶ˆëŸ¬ì˜¤ê¸° (token countìš©)
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# 6. í† í° ìˆ˜ ê³„ì‚° ë° json ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ìƒì„±
token_info = []
token_counts = []

for idx, doc in enumerate(split_docs):
    tokens = tokenizer.encode(doc.page_content, add_special_tokens=False)
    token_count = len(tokens)
    token_counts.append(token_count)

    chunk_data = {
        "chunk_index": idx + 1,
        "token_count": token_count,
        "text": doc.page_content,
        "metadata": doc.metadata,
    }
    token_info.append(chunk_data)

# 7. í‰ê·  í† í° ìˆ˜ ì¶œë ¥
avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
print(f"\nğŸ“Š Average tokens per chunk: {avg_tokens:.2f}")

# 8. JSON íŒŒì¼ë¡œ ì €ì¥
json_save_path = "./vectordb/token_info.json"
with open(json_save_path, "w", encoding="utf-8") as f:
    json.dump(token_info, f, indent=2, ensure_ascii=False)

print(f"ğŸ“ Token info saved to {json_save_path}")
